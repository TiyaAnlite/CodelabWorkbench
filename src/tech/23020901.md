---
title: PyTorch + CUDA环境配置
date: 2023-02-09
category: 技术向
tag:
    - 技术研究
    - 环境配置
isOriginal: true
---

其实具体的安装流程并不难，主要在于资源的收集以及版本的绑定上

从目前的情况来看，CUDA版本和PyTorch和Py自身版本是绑定的，而cuDNN和CUDA的大版本绑定，而其中主要受限的其实是PyTorch支持的CUDA版本，因此，这里采用CUDA来适配PyTorch的方式

<!-- more -->

## PyTorch安装

首先，下载PyTorch的本地安装包，需要下载的组件包有**Torch**，**TorchAudio**和**TorchVision**，要注意，这些组件包之间也是要对应版本的，不然在安装时就会跑去下其他版本的包了，一般来说都是版本完全对应，可能vision有差别可以用小版本对应，具体可以在安装时在命令行输出的版本依赖信息看到

[https://download.pytorch.org/whl/torch_stable.html](https://download.pytorch.org/whl/torch_stable.html)

在文件命名中，通常以`[组件包]-[版本]+cuxxx[CUDA版本]-cpxxx[Py版本]-cpxxx[Py版本]-win[操作系统]_amd64[架构].whl`来命名

- 组件包包含上文提到的**Torch**，**TorchAudio**和**TorchVision**

- `cu`后跟的是CUDA版本，例如11.7就是cu117

- Py版本需要对应你现在安装的Python版本

- 操作系统支持Linux和Windows，MacOS不支持CUDA版

- 平台架构支持amd64和x86_64

例如我这里现在的三个组件包对应Torch 1.13.1，将会是用于配置CUDA11.7，Python 3.10版本的

- torch-1.13.1+cu117-cp310-cp310-win_amd64.whl

- torchaudio-0.13.1+cu117-cp310-cp310-win_amd64.whl

- torchvision-0.14.1+cu117-cp310-cp310-win_amd64.whl

*这里也能下载到PyTorch，但是只有torch，建议用上面的*

*https://download.pytorch.org/whl/torch/*

> 当然，如果你对网络环境足够自信，也可以用[官网](https://pytorch.org/ "官网")的版本选择器来安装

最后，分别对三个组件包使用`pip install xxx.whl`进行安装

## CUDA安装

进入以下CUDA ToolKit页面进行下载

https://developer.nvidia.com/cuda-toolkit-archive

一般情况下，最新版本的CUDA是不一定不支持的，因此需要根据上面选择的Torch版本进行安装，小版本无所谓，主要是对应中版本，例如我选择的是`11.7.1`这个版本

下载后根据提示安装即可，其流程和显卡驱动安装流程一样

安装完成后，会自动添加环境变量，输入`nvcc -V`来确认CUDA编译器

```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Jun__8_16:59:34_Pacific_Daylight_Time_2022
Cuda compilation tools, release 11.7, V11.7.99
Build cuda_11.7.r11.7/compiler.31442593_0
```

## cuDNN安装

其实不能算是安装，cuDNN并没有单独的安装程序，其实是扩展在CUDA ToolKit里的相关库文件和头文件。找到cuDNN的下载页面

https://developer.nvidia.com/rdp/cudnn-archive

cuDNN的版本仅绑定CUDA的大版本，因此直接找大版本下的最新版本即可。下载压缩包后，分别解压其中的`bin`，`include`和`lib`到CUDA工具链的安装位置，默认工具链的位置为

```
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\
```

其中`v11.7`是你安装的CUDA版本，需要根据你实际安装的版本修改

然后添加以下环境变量到`path`中

```
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\lib\x64
```

同样其中的版本也要修改

最后定位到工具链的`extras\demo_suite`目录，运行`bandwidthTest.exe`和`deviceQuery.exe`，看到`Result = PASS`则安装完成

## TensorRT安装

因为后续做深度学习用到了[飞桨PaddlePaddle](https://www.paddlepaddle.org.cn/ "飞桨PaddlePaddle")，其推理库[Paddle-Inference](https://github.com/PaddlePaddle/Paddle-Inference-Demo/blob/master/docs/user_guides/download_lib.md "Paddle-Inference")在测试时发现需要TensorRT，因此也追加一个关于这个的安装，与上述cuDNN一样，都是一些库文件

https://developer.nvidia.com/nvidia-tensorrt-download

根据你用到的框架的版本要求，下载对应版本

解压`bin`, `include`到工具链的相应位置，同时解压`lib`中的`.dll`动态库到`bin`，`.lib`静态库到`lib`中

## 验证安装

``` python
import torch
# 返回当前设备索引
print(torch.cuda.current_device())
# 返回GPU的数量
print(torch.cuda.device_count())
# 返回gpu名字，设备索引默认从0开始
print(torch.cuda.get_device_name(0))
# cuda是否可用，为true就可用了
print(torch.cuda.is_available())
#查看cudnn版本
print(torch.backends.cudnn.version())
#查看torch版本
print(torch.__version__)
```
